{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA 3D Validation Features Extraction\n",
    "\n",
    "This notebook extracts all three types of features for RNA validation data:\n",
    "1. Thermodynamic features from RNA sequences\n",
    "2. Pseudodihedral angle features from 3D coordinates\n",
    "3. Mutual Information features from Multiple Sequence Alignments (MSAs)\n",
    "\n",
    "This notebook works with validation data that includes 3D structural information.\n",
    "\n",
    "## Dependencies\n",
    "- ViennaRNA (for thermodynamic features)\n",
    "- NumPy/SciPy/Pandas (core data processing)\n",
    "- Memory monitoring tools from src.analysis.memory_monitor\n",
    "- Feature extraction functions from src.analysis modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViennaRNA module imported successfully (version: 2.6.4)\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "\n",
    "# Ensure the parent directory is in the path so we can import our module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Import feature extraction modules\n",
    "from src.analysis.thermodynamic_analysis import extract_thermodynamic_features\n",
    "from src.analysis.dihedral_analysis import extract_dihedral_features\n",
    "from src.analysis.mutual_information import calculate_mutual_information, convert_mi_to_evolutionary_features\n",
    "from src.data.extract_features_simple import save_features_npz\n",
    "\n",
    "# Import memory monitoring utilities\n",
    "from src.analysis.memory_monitor import MemoryTracker, log_memory_usage, plot_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "# Define paths and parameters for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relative paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Output directories for each feature type\n",
    "THERMO_DIR = PROCESSED_DIR / \"thermo_features\"\n",
    "DIHEDRAL_DIR = PROCESSED_DIR / \"dihedral_features\"\n",
    "MI_DIR = PROCESSED_DIR / \"mi_features\"\n",
    "MEMORY_PLOTS_DIR = PROCESSED_DIR / \"memory_plots\"\n",
    "\n",
    "# Make sure all directories exist\n",
    "for directory in [RAW_DIR, PROCESSED_DIR, THERMO_DIR, DIHEDRAL_DIR, MI_DIR, MEMORY_PLOTS_DIR]:\n",
    "    directory.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "# Parameters\n",
    "LIMIT = 5  # Limit for testing; set to None to process all data\n",
    "VERBOSE = True  # Whether to print detailed progress\n",
    "\n",
    "# Auto-detect if running on Kaggle\n",
    "KAGGLE_MODE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "if KAGGLE_MODE:\n",
    "    print(\"Running in Kaggle environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Define utility functions for loading data and extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rna_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load RNA data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file containing RNA data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with RNA data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} entries from {csv_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_unique_target_ids(df, id_col=\"ID\"):\n",
    "    \"\"\"\n",
    "    Extract unique target IDs from dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with RNA data\n",
    "        id_col: Column containing IDs\n",
    "        \n",
    "    Returns:\n",
    "        List of unique target IDs\n",
    "    \"\"\"\n",
    "    # Extract target IDs (format: TARGET_ID_RESIDUE_NUM)\n",
    "    target_ids = []\n",
    "    for id_str in df[id_col]:\n",
    "        # Split the ID string and get the target ID part\n",
    "        parts = id_str.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            target_id = f\"{parts[0]}_{parts[1]}\"  # Take the first two parts (e.g., \"1SCL_A\")\n",
    "            target_ids.append(target_id)\n",
    "    \n",
    "    # Get unique target IDs\n",
    "    unique_targets = sorted(list(set(target_ids)))\n",
    "    print(f\"Found {len(unique_targets)} unique target IDs\")\n",
    "    return unique_targets\n",
    "\n",
    "def load_structure_data(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"\n",
    "    Load structure data for a given target from labels CSV.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        data_dir: Directory containing data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with structure coordinates or None if not found\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Extract base ID without residue number\n",
    "    parts = target_id.split('_')\n",
    "    base_id = parts[0]\n",
    "    \n",
    "    # Define possible label files\n",
    "    label_files = [\n",
    "        data_dir / \"validation_labels.csv\",\n",
    "        data_dir / \"test_labels.csv\"\n",
    "    ]\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        if label_file.exists():\n",
    "            try:\n",
    "                print(f\"Looking for structure data for {target_id} in {label_file}\")\n",
    "                # Read the entire CSV file\n",
    "                all_data = pd.read_csv(label_file)\n",
    "                \n",
    "                # Method 1: Try exact match\n",
    "                if \"ID\" in all_data.columns:\n",
    "                    target_data = all_data[all_data[\"ID\"].str.startswith(f\"{target_id}_\")]\n",
    "                    if len(target_data) > 0:\n",
    "                        print(f\"Found {len(target_data)} residues for {target_id}\")\n",
    "                        return target_data\n",
    "                \n",
    "                # Method 2: Try matching by base ID\n",
    "                if \"ID\" in all_data.columns:\n",
    "                    # Look for rows with this base ID\n",
    "                    target_data = all_data[all_data[\"ID\"].str.startswith(f\"{base_id}_\")]\n",
    "                    if len(target_data) > 0:\n",
    "                        print(f\"Found {len(target_data)} residues with base ID {base_id}\")\n",
    "                        return target_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading from {label_file}: {e}\")\n",
    "    \n",
    "    print(f\"Could not find structure data for {target_id} in any labels file\")\n",
    "    return None\n",
    "\n",
    "def load_msa_data(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"\n",
    "    Load MSA data for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        data_dir: Directory containing MSA data\n",
    "        \n",
    "    Returns:\n",
    "        List of MSA sequences or None if not found\n",
    "    \"\"\"\n",
    "    # Extract base ID without residue number\n",
    "    base_id = target_id.split('_')[0]\n",
    "    \n",
    "    # Define possible MSA directories and extensions\n",
    "    msa_dirs = [\n",
    "        data_dir / \"MSA\",\n",
    "        data_dir,\n",
    "        data_dir / \"alignments\",\n",
    "        data_dir / \"msa\",\n",
    "        data_dir / \"validation\" / \"MSA\",\n",
    "        data_dir / \"validation\"\n",
    "    ]\n",
    "    \n",
    "    extensions = [\".MSA.fasta\", \".fasta\", \".fa\", \".afa\", \".msa\"]\n",
    "    \n",
    "    # Try with base ID and original target ID\n",
    "    for id_to_try in [base_id, target_id]:\n",
    "        for msa_dir in msa_dirs:\n",
    "            if not msa_dir.exists():\n",
    "                continue\n",
    "                \n",
    "            for ext in extensions:\n",
    "                msa_path = msa_dir / f\"{id_to_try}{ext}\"\n",
    "                if msa_path.exists():\n",
    "                    print(f\"Loading MSA data from {msa_path}\")\n",
    "                    try:\n",
    "                        # Parse FASTA file\n",
    "                        sequences = []\n",
    "                        current_seq = \"\"\n",
    "                        \n",
    "                        with open(msa_path, 'r') as f:\n",
    "                            for line in f:\n",
    "                                line = line.strip()\n",
    "                                if line.startswith('>'):\n",
    "                                    if current_seq:\n",
    "                                        sequences.append(current_seq)\n",
    "                                        current_seq = \"\"\n",
    "                                else:\n",
    "                                    current_seq += line\n",
    "                                    \n",
    "                            # Add the last sequence\n",
    "                            if current_seq:\n",
    "                                sequences.append(current_seq)\n",
    "                        \n",
    "                        print(f\"Loaded {len(sequences)} sequences from MSA\")\n",
    "                        return sequences\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading MSA data: {e}\")\n",
    "    \n",
    "    # Last resort: recursive search for files containing the base ID\n",
    "    try:\n",
    "        for pattern in [f\"**/{base_id}*.fasta\", f\"**/{base_id}*.fa\", f\"**/{base_id}*.msa\"]:\n",
    "            matches = list(data_dir.glob(pattern))\n",
    "            if matches:\n",
    "                msa_path = matches[0]\n",
    "                print(f\"Found MSA via recursive search: {msa_path}\")\n",
    "                \n",
    "                # Parse the file\n",
    "                sequences = []\n",
    "                current_seq = \"\"\n",
    "                \n",
    "                with open(msa_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line.startswith('>'):\n",
    "                            if current_seq:\n",
    "                                sequences.append(current_seq)\n",
    "                                current_seq = \"\"\n",
    "                        else:\n",
    "                            current_seq += line\n",
    "                            \n",
    "                    # Add the last sequence\n",
    "                    if current_seq:\n",
    "                        sequences.append(current_seq)\n",
    "                \n",
    "                print(f\"Loaded {len(sequences)} sequences from MSA\")\n",
    "                return sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error in recursive MSA search: {e}\")\n",
    "    \n",
    "    print(f\"Could not find MSA data for {target_id}\")\n",
    "    return None\n",
    "\n",
    "def get_sequence_for_target(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"\n",
    "    Get RNA sequence for a target ID from the sequence file.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID (e.g., \"R1107_A\")\n",
    "        data_dir: Directory containing sequence data\n",
    "        \n",
    "    Returns:\n",
    "        RNA sequence as string or None if not found\n",
    "    \"\"\"\n",
    "    # Try validation_sequences.csv first\n",
    "    validation_seq_path = data_dir / \"validation_sequences.csv\"\n",
    "    if validation_seq_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(validation_seq_path)\n",
    "            # Try different possible column names for ID and sequence\n",
    "            id_cols = [\"target_id\", \"ID\", \"id\"]\n",
    "            seq_cols = [\"sequence\", \"Sequence\", \"seq\"]\n",
    "            \n",
    "            for id_col in id_cols:\n",
    "                if id_col in df.columns:\n",
    "                    for seq_col in seq_cols:\n",
    "                        if seq_col in df.columns:\n",
    "                            # Try exact match first\n",
    "                            target_row = df[df[id_col] == target_id]\n",
    "                            if len(target_row) > 0:\n",
    "                                sequence = target_row[seq_col].iloc[0]\n",
    "                                print(f\"Found sequence for {target_id} in validation_sequences.csv (exact match), length: {len(sequence)}\")\n",
    "                                return sequence\n",
    "                            \n",
    "                            # If exact match fails, try base target ID (remove residue number)\n",
    "                            base_id = target_id.split('_')[0]\n",
    "                            if '_' in target_id:\n",
    "                                # Try with just first component\n",
    "                                target_row = df[df[id_col] == base_id]\n",
    "                                if len(target_row) > 0:\n",
    "                                    sequence = target_row[seq_col].iloc[0]\n",
    "                                    print(f\"Found sequence for {target_id} using base ID {base_id}, length: {len(sequence)}\")\n",
    "                                    return sequence\n",
    "                                \n",
    "                                # Try with first two components (target and chain)\n",
    "                                parts = target_id.split('_')\n",
    "                                if len(parts) >= 2:\n",
    "                                    base_id_with_chain = f\"{parts[0]}_{parts[1]}\"\n",
    "                                    target_row = df[df[id_col] == base_id_with_chain]\n",
    "                                    if len(target_row) > 0:\n",
    "                                        sequence = target_row[seq_col].iloc[0]\n",
    "                                        print(f\"Found sequence for {target_id} using base+chain ID {base_id_with_chain}, length: {len(sequence)}\")\n",
    "                                        return sequence\n",
    "                            \n",
    "                            # Try partial matching\n",
    "                            for row_id in df[id_col]:\n",
    "                                if str(row_id) in target_id or target_id.startswith(str(row_id)):\n",
    "                                    target_row = df[df[id_col] == row_id]\n",
    "                                    sequence = target_row[seq_col].iloc[0]\n",
    "                                    print(f\"Found sequence for {target_id} using partial match with {row_id}, length: {len(sequence)}\")\n",
    "                                    return sequence\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sequence data from {validation_seq_path}: {e}\")\n",
    "    \n",
    "    # Continue with the rest of your existing function...\n",
    "    \n",
    "    # If not found in validation_sequences.csv, try other sequence files\n",
    "    sequence_paths = [\n",
    "        data_dir / \"sequences.csv\",\n",
    "        data_dir / \"test_sequences.csv\",\n",
    "        data_dir / \"validation_sequences.csv\"\n",
    "    ]\n",
    "    \n",
    "    for path in sequence_paths:\n",
    "        if path.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                \n",
    "                # Try different possible column names\n",
    "                id_cols = [\"target_id\", \"ID\", \"id\"]\n",
    "                seq_cols = [\"sequence\", \"Sequence\", \"seq\"]\n",
    "                \n",
    "                for id_col in id_cols:\n",
    "                    if id_col in df.columns:\n",
    "                        for seq_col in seq_cols:\n",
    "                            if seq_col in df.columns:\n",
    "                                # Find the target in the dataframe\n",
    "                                target_row = df[df[id_col] == target_id]\n",
    "                                if len(target_row) > 0:\n",
    "                                    sequence = target_row[seq_col].iloc[0]\n",
    "                                    print(f\"Found sequence for {target_id} in {path.name}, length: {len(sequence)}\")\n",
    "                                    return sequence\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading sequence data from {path}: {e}\")\n",
    "    \n",
    "    # As a last resort, try to extract it from MSA data\n",
    "    print(f\"Could not find sequence in CSV files, trying MSA files as a last resort\")\n",
    "    msa_sequences = load_msa_data(target_id, data_dir)\n",
    "    if msa_sequences and len(msa_sequences) > 0:\n",
    "        # The first sequence in the MSA is typically the target sequence\n",
    "        sequence = msa_sequences[0]\n",
    "        print(f\"Found sequence for {target_id} in MSA file, length: {len(sequence)}\")\n",
    "        return sequence\n",
    "    \n",
    "    print(f\"Could not find sequence for {target_id} in any file\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Functions\n",
    "\n",
    "# Define functions for extracting each type of feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_thermo_features_for_target(target_id, sequence=None):\n",
    "    \"\"\"\n",
    "    Extract thermodynamic features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        sequence: RNA sequence (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with thermodynamic features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting thermodynamic features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get sequence if not provided\n",
    "        if sequence is None:\n",
    "            sequence = get_sequence_for_target(target_id)\n",
    "            if sequence is None:\n",
    "                print(f\"Failed to get sequence for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Log initial memory usage\n",
    "        log_memory_usage(f\"Before thermo features for {target_id} (len={len(sequence)})\")\n",
    "        \n",
    "        # Calculate features with memory monitoring\n",
    "        print(f\"Calculating thermodynamic features for sequence of length {len(sequence)}\")\n",
    "        with MemoryTracker(f\"Thermodynamic features calculation for {target_id}\"):\n",
    "            features = extract_thermodynamic_features(sequence)\n",
    "        \n",
    "        # Save features\n",
    "        output_file = THERMO_DIR / f\"{target_id}_thermo_features.npz\"\n",
    "        features['target_id'] = target_id\n",
    "        features['sequence'] = sequence\n",
    "        \n",
    "        with MemoryTracker(\"Saving thermodynamic features\"):\n",
    "            save_features_npz(features, output_file)\n",
    "        \n",
    "        # Log final memory usage\n",
    "        log_memory_usage(f\"After thermo features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted thermodynamic features in {elapsed_time:.2f} seconds\")\n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting thermodynamic features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def extract_dihedral_features_for_target(target_id, structure_data=None):\n",
    "    \"\"\"\n",
    "    Extract pseudodihedral angle features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure coordinates (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with dihedral features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting dihedral features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get structure data if not provided\n",
    "        if structure_data is None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "            if structure_data is None:\n",
    "                print(f\"Failed to get structure data for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Check if we have at least 4 residues (required for dihedral angles)\n",
    "        if len(structure_data) < 4:\n",
    "            print(f\"Not enough residues ({len(structure_data)}) for {target_id}, minimum 4 required for dihedral angles\")\n",
    "            return None\n",
    "        \n",
    "        # Check if we have the necessary coordinate columns\n",
    "        required_cols = ['x_1', 'y_1', 'z_1']\n",
    "        if not all(col in structure_data.columns for col in required_cols):\n",
    "            # Try to find alternative column names\n",
    "            alt_x_cols = [col for col in structure_data.columns if col.startswith('x_')]\n",
    "            if alt_x_cols:\n",
    "                x_col = alt_x_cols[0]\n",
    "                y_col = x_col.replace('x_', 'y_')\n",
    "                z_col = x_col.replace('x_', 'z_')\n",
    "                \n",
    "                # Rename columns for compatibility\n",
    "                if all(col in structure_data.columns for col in [x_col, y_col, z_col]):\n",
    "                    structure_data = structure_data.rename(columns={\n",
    "                        x_col: 'x_1',\n",
    "                        y_col: 'y_1',\n",
    "                        z_col: 'z_1'\n",
    "                    })\n",
    "                    print(f\"Renamed columns {x_col}, {y_col}, {z_col} to x_1, y_1, z_1\")\n",
    "                else:\n",
    "                    print(f\"Missing coordinate columns for {target_id}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Missing coordinate columns for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Log memory before dihedral calculation\n",
    "        log_memory_usage(f\"Before dihedral features for {target_id} (residues={len(structure_data)})\")\n",
    "        \n",
    "        # Calculate dihedral features\n",
    "        output_file = DIHEDRAL_DIR / f\"{target_id}_dihedral_features.npz\"\n",
    "        print(f\"Calculating dihedral features for {len(structure_data)} residues\")\n",
    "        \n",
    "        with MemoryTracker(f\"Dihedral features calculation for {target_id}\"):\n",
    "            dihedral_features = extract_dihedral_features(structure_data, output_file=output_file, include_raw_angles=True)\n",
    "        \n",
    "        # Log memory after dihedral calculation\n",
    "        log_memory_usage(f\"After dihedral features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted dihedral features in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Add target ID\n",
    "        dihedral_features['target_id'] = target_id\n",
    "        return dihedral_features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dihedral features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def extract_mi_features_for_target(target_id, structure_data=None, msa_sequences=None):\n",
    "    \"\"\"\n",
    "    Extract Mutual Information features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure data for correlation calculation (optional)\n",
    "        msa_sequences: List of MSA sequences (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MI features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting MI features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get MSA sequences if not provided\n",
    "        if msa_sequences is None:\n",
    "            msa_sequences = load_msa_data(target_id)\n",
    "            if msa_sequences is None or len(msa_sequences) < 2:\n",
    "                print(f\"Failed to get MSA data for {target_id} or not enough sequences\")\n",
    "                return None\n",
    "        \n",
    "        # Get structure data if not provided (for correlation calculation)\n",
    "        if structure_data is None and target_id is not None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "        \n",
    "        # Log memory before MI calculation\n",
    "        sequence_length = len(msa_sequences[0]) if msa_sequences else 0\n",
    "        msa_size = len(msa_sequences) if msa_sequences else 0\n",
    "        log_memory_usage(f\"Before MI features for {target_id} (seq_len={sequence_length}, msa_size={msa_size})\")\n",
    "        \n",
    "        # Calculate MI (this may take some time for large MSAs)\n",
    "        print(f\"Calculating MI for {len(msa_sequences)} sequences\")\n",
    "        with MemoryTracker(f\"MI calculation for {target_id}\"):\n",
    "            mi_result = calculate_mutual_information(msa_sequences, verbose=VERBOSE)\n",
    "        \n",
    "        if mi_result is None:\n",
    "            print(f\"Failed to calculate MI for {target_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to evolutionary features\n",
    "        output_file = MI_DIR / f\"{target_id}_mi_features.npz\"\n",
    "        \n",
    "        # If we have structure data, use it for correlation calculation\n",
    "        if structure_data is not None:\n",
    "            print(f\"Converting MI to evolutionary features with structural correlation\")\n",
    "            with MemoryTracker(f\"MI-structure correlation for {target_id}\"):\n",
    "                features = convert_mi_to_evolutionary_features(mi_result, structure_data, output_file=output_file)\n",
    "        else:\n",
    "            print(f\"Converting MI to evolutionary features without structural correlation\")\n",
    "            features = mi_result\n",
    "            \n",
    "            # Save manually if convert_mi_to_evolutionary_features wasn't used\n",
    "            if output_file is not None:\n",
    "                with MemoryTracker(f\"Saving MI features for {target_id}\"):\n",
    "                    np.savez_compressed(output_file, **features)\n",
    "                print(f\"Saved MI features to {output_file}\")\n",
    "        \n",
    "        # Log memory after MI calculation\n",
    "        log_memory_usage(f\"After MI features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted MI features in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Add target ID\n",
    "        features['target_id'] = target_id\n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting MI features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dihedral_features_for_all_structures(target_id, structure_data=None):\n",
    "    \"\"\"\n",
    "    Extract pseudodihedral angle features for all structure sets in validation data.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure coordinates\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with dihedral features for all structures or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting dihedral features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get structure data if not provided\n",
    "        if structure_data is None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "            if structure_data is None:\n",
    "                print(f\"Failed to get structure data for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Check if we have at least 4 residues (required for dihedral angles)\n",
    "        if len(structure_data) < 4:\n",
    "            print(f\"Not enough residues ({len(structure_data)}) for {target_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Find all coordinate sets (x_1, y_1, z_1), (x_2, y_2, z_2), etc.\n",
    "        x_cols = sorted([col for col in structure_data.columns if col.startswith('x_')])\n",
    "        y_cols = sorted([col for col in structure_data.columns if col.startswith('y_')])\n",
    "        z_cols = sorted([col for col in structure_data.columns if col.startswith('z_')])\n",
    "        \n",
    "        # Make sure we have matching coordinate sets\n",
    "        num_structures = len(x_cols)\n",
    "        if not (len(x_cols) == len(y_cols) == len(z_cols)):\n",
    "            print(f\"Mismatched coordinate columns: {len(x_cols)} x-cols, {len(y_cols)} y-cols, {len(z_cols)} z-cols\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Found {num_structures} structure sets for {target_id}\")\n",
    "        \n",
    "        # Output file path\n",
    "        output_file = DIHEDRAL_DIR / f\"{target_id}_dihedral_features.npz\"\n",
    "        \n",
    "        # Process each structure set\n",
    "        all_features = {\n",
    "            'target_id': target_id,\n",
    "            'num_structures': num_structures,\n",
    "            'structure_ids': list(range(1, num_structures + 1))\n",
    "        }\n",
    "        \n",
    "        for i in range(num_structures):\n",
    "            # Get column names for this structure\n",
    "            struct_idx = i + 1  # 1-based indexing in column names\n",
    "            x_col = f'x_{struct_idx}'\n",
    "            y_col = f'y_{struct_idx}'\n",
    "            z_col = f'z_{struct_idx}'\n",
    "            \n",
    "            # Skip if any column is missing\n",
    "            if not all(col in structure_data.columns for col in [x_col, y_col, z_col]):\n",
    "                print(f\"Skipping structure {struct_idx} due to missing coordinates\")\n",
    "                continue\n",
    "            \n",
    "            # Create a copy with renamed columns for compatibility with dihedral_analysis\n",
    "            struct_data = structure_data.copy()\n",
    "            \n",
    "            # Create 'resid' column if not present\n",
    "            if 'resid' not in struct_data.columns:\n",
    "                if 'residue' in struct_data.columns:\n",
    "                    struct_data['resid'] = struct_data['residue']\n",
    "                else:\n",
    "                    struct_data['resid'] = list(range(1, len(struct_data) + 1))\n",
    "            \n",
    "            # Rename coordinate columns to the standard names expected by dihedral_analysis\n",
    "            struct_data = struct_data.rename(columns={\n",
    "                x_col: 'x_1',\n",
    "                y_col: 'y_1',\n",
    "                z_col: 'z_1'\n",
    "            })\n",
    "            \n",
    "            print(f\"Processing structure {struct_idx}/{num_structures}\")\n",
    "            \n",
    "            # Calculate dihedral features for this structure\n",
    "            try:\n",
    "                dihedral_features = extract_dihedral_features(\n",
    "                    struct_data, \n",
    "                    output_file=None,  # Don't save individual structures\n",
    "                    include_raw_angles=True\n",
    "                )\n",
    "                \n",
    "                if dihedral_features is not None:\n",
    "                    # Add prefixed keys to identify this structure set\n",
    "                    for key, value in dihedral_features.items():\n",
    "                        all_features[f'struct_{struct_idx}_{key}'] = value\n",
    "                    \n",
    "                    print(f\"✅ Successfully processed structure {struct_idx}\")\n",
    "                else:\n",
    "                    print(f\"❌ Failed to extract features for structure {struct_idx}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing structure {struct_idx}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Check if we have at least one successful structure\n",
    "        successful_structs = sum(1 for key in all_features.keys() if key.startswith('struct_'))\n",
    "        if successful_structs == 0:\n",
    "            print(f\"No structures were successfully processed for {target_id}\")\n",
    "            return None\n",
    "            \n",
    "        # Save combined features\n",
    "        print(f\"Saving combined features for {successful_structs}/{num_structures} structures\")\n",
    "        save_features_npz(all_features, output_file)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Completed dihedral extraction in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in dihedral extraction for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_dihedral_features_for_target(target_id, structure_data=None):\n",
    "    \"\"\"\n",
    "    Extract pseudodihedral angle features for a given target with multiple coordinate sets.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure coordinates (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with dihedral features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting dihedral features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get structure data if not provided\n",
    "        if structure_data is None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "            if structure_data is None:\n",
    "                print(f\"Failed to get structure data for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Find coordinate sets\n",
    "        x_cols = sorted([col for col in structure_data.columns if col.startswith('x_')])\n",
    "        \n",
    "        # Check how many residues we have\n",
    "        num_residues = len(structure_data)\n",
    "        print(f\"Found {num_residues} residues and {len(x_cols)} coordinate sets\")\n",
    "        \n",
    "        if num_residues < 4:\n",
    "            print(f\"Not enough residues ({num_residues}) for {target_id}\")\n",
    "            return None\n",
    "            \n",
    "        # Initialize result dictionary\n",
    "        result_features = {\n",
    "            'target_id': target_id,\n",
    "            'num_structures': 0,\n",
    "            'structure_ids': []\n",
    "        }\n",
    "        \n",
    "        # Process each coordinate set\n",
    "        for coord_set in range(1, len(x_cols) + 1):\n",
    "            x_col = f'x_{coord_set}'\n",
    "            y_col = f'y_{coord_set}'\n",
    "            z_col = f'z_{coord_set}'\n",
    "            \n",
    "            # Skip if any of these columns doesn't exist\n",
    "            if not all(col in structure_data.columns for col in [x_col, y_col, z_col]):\n",
    "                continue\n",
    "                \n",
    "            # Check if there are valid coordinates (not -1e+18)\n",
    "            if ((structure_data[x_col] < -1e17) | (structure_data[x_col] > 1e17)).all():\n",
    "                print(f\"Skipping coordinate set {coord_set} (no valid coordinates)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing coordinate set {coord_set}\")\n",
    "            \n",
    "            # Create dataframe with this coordinate set\n",
    "            temp_df = structure_data.copy()\n",
    "            \n",
    "            # Rename columns to the standard x_1, y_1, z_1 expected by the dihedral function\n",
    "            temp_df = temp_df.rename(columns={\n",
    "                x_col: 'x_1',\n",
    "                y_col: 'y_1',\n",
    "                z_col: 'z_1'\n",
    "            })\n",
    "            \n",
    "            # Make sure resid column exists\n",
    "            if 'resid' not in temp_df.columns:\n",
    "                if 'residue' in temp_df.columns:\n",
    "                    temp_df['resid'] = temp_df['residue']\n",
    "                else:\n",
    "                    temp_df['resid'] = range(1, len(temp_df) + 1)\n",
    "            \n",
    "            # Calculate dihedral features\n",
    "            dihedral_features = extract_dihedral_features(\n",
    "                temp_df,\n",
    "                output_file=None,\n",
    "                include_raw_angles=True\n",
    "            )\n",
    "            \n",
    "            if dihedral_features is not None:\n",
    "                # Store features with a prefix indicating the coordinate set\n",
    "                for key, value in dihedral_features.items():\n",
    "                    result_features[f'struct_{coord_set}_{key}'] = value\n",
    "                \n",
    "                # Update metadata\n",
    "                result_features['num_structures'] += 1\n",
    "                result_features['structure_ids'].append(coord_set)\n",
    "                \n",
    "                print(f\"✅ Successfully processed coordinate set {coord_set}\")\n",
    "            else:\n",
    "                print(f\"❌ Failed to extract dihedral features for coordinate set {coord_set}\")\n",
    "        \n",
    "        # Save output\n",
    "        if result_features['num_structures'] > 0:\n",
    "            output_file = DIHEDRAL_DIR / f\"{target_id}_dihedral_features.npz\"\n",
    "            save_features_npz(result_features, output_file)\n",
    "            print(f\"Saved dihedral features for {result_features['num_structures']} coordinate sets\")\n",
    "            return result_features\n",
    "        else:\n",
    "            print(f\"No valid coordinate sets found for {target_id}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dihedral features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "def extract_mi_features_for_target(target_id, structure_data=None, msa_sequences=None):\n",
    "    \"\"\"\n",
    "    Extract Mutual Information features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure data for correlation calculation (optional)\n",
    "        msa_sequences: List of MSA sequences (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MI features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting MI features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get MSA sequences if not provided\n",
    "        if msa_sequences is None:\n",
    "            msa_sequences = load_msa_data(target_id)\n",
    "            if msa_sequences is None or len(msa_sequences) < 2:\n",
    "                print(f\"Failed to get MSA data for {target_id} or not enough sequences\")\n",
    "                return None\n",
    "        \n",
    "        # Get structure data if not provided (for correlation calculation)\n",
    "        if structure_data is None and target_id is not None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "        \n",
    "        # Log memory before MI calculation\n",
    "        sequence_length = len(msa_sequences[0]) if msa_sequences else 0\n",
    "        msa_size = len(msa_sequences) if msa_sequences else 0\n",
    "        log_memory_usage(f\"Before MI features for {target_id} (seq_len={sequence_length}, msa_size={msa_size})\")\n",
    "        \n",
    "        # Calculate MI (this may take some time for large MSAs)\n",
    "        print(f\"Calculating MI for {len(msa_sequences)} sequences\")\n",
    "        with MemoryTracker(f\"MI calculation for {target_id}\"):\n",
    "            mi_result = calculate_mutual_information(msa_sequences, verbose=VERBOSE)\n",
    "        \n",
    "        if mi_result is None:\n",
    "            print(f\"Failed to calculate MI for {target_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to evolutionary features\n",
    "        output_file = MI_DIR / f\"{target_id}_mi_features.npz\"\n",
    "        \n",
    "        # If we have structure data, use it for correlation calculation\n",
    "        if structure_data is not None:\n",
    "            print(f\"Converting MI to evolutionary features with structural correlation\")\n",
    "            with MemoryTracker(f\"MI-structure correlation for {target_id}\"):\n",
    "                features = convert_mi_to_evolutionary_features(mi_result, structure_data, output_file=output_file)\n",
    "        else:\n",
    "            print(f\"Converting MI to evolutionary features without structural correlation\")\n",
    "            features = mi_result\n",
    "            \n",
    "            # Save manually if convert_mi_to_evolutionary_features wasn't used\n",
    "            if output_file is not None:\n",
    "                with MemoryTracker(f\"Saving MI features for {target_id}\"):\n",
    "                    np.savez_compressed(output_file, **features)\n",
    "                print(f\"Saved MI features to {output_file}\")\n",
    "        \n",
    "        # Log memory after MI calculation\n",
    "        log_memory_usage(f\"After MI features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted MI features in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Add target ID\n",
    "        features['target_id'] = target_id\n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting MI features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dihedral extraction for validation target: R1107\n",
      "Looking for structure data for R1107 in ../data/raw/validation_labels.csv\n",
      "Found 69 residues for R1107\n",
      "Found 40 coordinate sets: x_1, x_2, x_3, x_4, x_5...\n",
      "Extracting dihedral features for R1107\n",
      "Found 69 residues and 40 coordinate sets\n",
      "Processing coordinate set 1\n",
      "✅ Successfully processed coordinate set 1\n",
      "Skipping coordinate set 2 (no valid coordinates)\n",
      "Skipping coordinate set 3 (no valid coordinates)\n",
      "Skipping coordinate set 4 (no valid coordinates)\n",
      "Skipping coordinate set 5 (no valid coordinates)\n",
      "Skipping coordinate set 6 (no valid coordinates)\n",
      "Skipping coordinate set 7 (no valid coordinates)\n",
      "Skipping coordinate set 8 (no valid coordinates)\n",
      "Skipping coordinate set 9 (no valid coordinates)\n",
      "Skipping coordinate set 10 (no valid coordinates)\n",
      "Skipping coordinate set 11 (no valid coordinates)\n",
      "Skipping coordinate set 12 (no valid coordinates)\n",
      "Skipping coordinate set 13 (no valid coordinates)\n",
      "Skipping coordinate set 14 (no valid coordinates)\n",
      "Skipping coordinate set 15 (no valid coordinates)\n",
      "Skipping coordinate set 16 (no valid coordinates)\n",
      "Skipping coordinate set 17 (no valid coordinates)\n",
      "Skipping coordinate set 18 (no valid coordinates)\n",
      "Skipping coordinate set 19 (no valid coordinates)\n",
      "Skipping coordinate set 20 (no valid coordinates)\n",
      "Skipping coordinate set 21 (no valid coordinates)\n",
      "Skipping coordinate set 22 (no valid coordinates)\n",
      "Skipping coordinate set 23 (no valid coordinates)\n",
      "Skipping coordinate set 24 (no valid coordinates)\n",
      "Skipping coordinate set 25 (no valid coordinates)\n",
      "Skipping coordinate set 26 (no valid coordinates)\n",
      "Skipping coordinate set 27 (no valid coordinates)\n",
      "Skipping coordinate set 28 (no valid coordinates)\n",
      "Skipping coordinate set 29 (no valid coordinates)\n",
      "Skipping coordinate set 30 (no valid coordinates)\n",
      "Skipping coordinate set 31 (no valid coordinates)\n",
      "Skipping coordinate set 32 (no valid coordinates)\n",
      "Skipping coordinate set 33 (no valid coordinates)\n",
      "Skipping coordinate set 34 (no valid coordinates)\n",
      "Skipping coordinate set 35 (no valid coordinates)\n",
      "Skipping coordinate set 36 (no valid coordinates)\n",
      "Skipping coordinate set 37 (no valid coordinates)\n",
      "Skipping coordinate set 38 (no valid coordinates)\n",
      "Skipping coordinate set 39 (no valid coordinates)\n",
      "Skipping coordinate set 40 (no valid coordinates)\n",
      "Saved features to ../data/processed/dihedral_features/R1107_dihedral_features.npz\n",
      "Saved dihedral features for 1 coordinate sets\n",
      "✅ Successfully extracted dihedral features\n",
      "Number of structures: 1\n",
      "Feature keys examples:\n",
      "  - target_id\n",
      "  - num_structures\n",
      "  - structure_ids\n",
      "  - struct_1_features\n",
      "  - struct_1_feature_names\n"
     ]
    }
   ],
   "source": [
    "def test_validation_dihedral_extraction(test_target=\"R1107\"):  # Use a default test target\n",
    "    \"\"\"Test dihedral extraction on a single validation target.\"\"\"\n",
    "    print(f\"Testing dihedral extraction for validation target: {test_target}\")\n",
    "    \n",
    "    # Load structure data\n",
    "    structure_data = load_structure_data(test_target)\n",
    "    if structure_data is None:\n",
    "        print(f\"❌ Failed to load structure data for {test_target}\")\n",
    "        return\n",
    "    \n",
    "    # Count coordinate sets\n",
    "    x_cols = [col for col in structure_data.columns if col.startswith('x_')]\n",
    "    print(f\"Found {len(x_cols)} coordinate sets: {', '.join(x_cols[:5])}...\")\n",
    "    \n",
    "    # Extract dihedral features\n",
    "    features = extract_dihedral_features_for_target(test_target, structure_data)\n",
    "    \n",
    "    if features is not None:\n",
    "        print(f\"✅ Successfully extracted dihedral features\")\n",
    "        print(f\"Number of structures: {features.get('num_structures', 'Unknown')}\")\n",
    "        print(\"Feature keys examples:\")\n",
    "        for key in list(features.keys())[:5]:\n",
    "            print(f\"  - {key}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to extract dihedral features\")\n",
    "\n",
    "# Run the test with a specific RNA target ID from the validation set\n",
    "test_validation_dihedral_extraction(\"R1107\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process multiple targets in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_target(target_id, extract_thermo=True, extract_dihedral=True, extract_mi=True):\n",
    "    \"\"\"\n",
    "    Process a single target, extracting all requested feature types.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        extract_thermo: Whether to extract thermodynamic features\n",
    "        extract_dihedral: Whether to extract dihedral features\n",
    "        extract_mi: Whether to extract MI features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each feature type\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing target: {target_id}\")\n",
    "    results = {'target_id': target_id}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load common data that might be used by multiple feature types\n",
    "    sequence = get_sequence_for_target(target_id) if extract_thermo else None\n",
    "    structure_data = load_structure_data(target_id) if extract_dihedral or extract_mi else None\n",
    "    msa_sequences = load_msa_data(target_id) if extract_mi else None\n",
    "    \n",
    "    # Extract thermodynamic features\n",
    "    if extract_thermo:\n",
    "        thermo_file = THERMO_DIR / f\"{target_id}_thermo_features.npz\"\n",
    "        \n",
    "        if thermo_file.exists():\n",
    "            print(f\"Thermodynamic features already exist for {target_id}\")\n",
    "            results['thermo'] = {'success': True, 'skipped': True}\n",
    "        else:\n",
    "            thermo_features = extract_thermo_features_for_target(target_id, sequence)\n",
    "            results['thermo'] = {'success': thermo_features is not None}\n",
    "    \n",
    "    # Extract dihedral features\n",
    "    if extract_dihedral:\n",
    "        dihedral_file = DIHEDRAL_DIR / f\"{target_id}_dihedral_features.npz\"\n",
    "        \n",
    "        if dihedral_file.exists():\n",
    "            print(f\"Dihedral features already exist for {target_id}\")\n",
    "            results['dihedral'] = {'success': True, 'skipped': True}\n",
    "        else:\n",
    "            dihedral_features = extract_dihedral_features_for_target(target_id, structure_data)\n",
    "            results['dihedral'] = {'success': dihedral_features is not None}\n",
    "    \n",
    "    # Extract MI features\n",
    "    if extract_mi:\n",
    "        mi_file = MI_DIR / f\"{target_id}_mi_features.npz\"\n",
    "        \n",
    "        if mi_file.exists():\n",
    "            print(f\"MI features already exist for {target_id}\")\n",
    "            results['mi'] = {'success': True, 'skipped': True}\n",
    "        else:\n",
    "            mi_features = extract_mi_features_for_target(target_id, structure_data, msa_sequences)\n",
    "            results['mi'] = {'success': mi_features is not None}\n",
    "    \n",
    "    # Calculate total time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    results['elapsed_time'] = elapsed_time\n",
    "    print(f\"Completed processing {target_id} in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def batch_process_targets(target_ids, extract_thermo=True, extract_dihedral=True, extract_mi=True):\n",
    "    \"\"\"\n",
    "    Process multiple targets in batch mode.\n",
    "    \n",
    "    Args:\n",
    "        target_ids: List of target IDs\n",
    "        extract_thermo: Whether to extract thermodynamic features\n",
    "        extract_dihedral: Whether to extract dihedral features\n",
    "        extract_mi: Whether to extract MI features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each target\n",
    "    \"\"\"\n",
    "    print(f\"Starting batch processing for {len(target_ids)} targets\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = {}\n",
    "    for i, target_id in enumerate(target_ids):\n",
    "        print(f\"\\nProcessing target {i+1}/{len(target_ids)}: {target_id}\")\n",
    "        \n",
    "        # Process the target\n",
    "        target_results = process_target(\n",
    "            target_id, \n",
    "            extract_thermo=extract_thermo, \n",
    "            extract_dihedral=extract_dihedral, \n",
    "            extract_mi=extract_mi\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[target_id] = target_results\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    success_counts = {\n",
    "        'thermo': sum(1 for r in results.values() if 'thermo' in r and r['thermo']['success']),\n",
    "        'dihedral': sum(1 for r in results.values() if 'dihedral' in r and r['dihedral']['success']),\n",
    "        'mi': sum(1 for r in results.values() if 'mi' in r and r['mi']['success'])\n",
    "    }\n",
    "    \n",
    "    skipped_counts = {\n",
    "        'thermo': sum(1 for r in results.values() if 'thermo' in r and r['thermo'].get('skipped', False)),\n",
    "        'dihedral': sum(1 for r in results.values() if 'dihedral' in r and r['dihedral'].get('skipped', False)),\n",
    "        'mi': sum(1 for r in results.values() if 'mi' in r and r['mi'].get('skipped', False))\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nBatch processing complete!\")\n",
    "    print(f\"Total targets: {len(target_ids)}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if extract_thermo:\n",
    "        print(f\"Thermodynamic features: {success_counts['thermo']} successful ({skipped_counts['thermo']} skipped)\")\n",
    "        \n",
    "    if extract_dihedral:\n",
    "        print(f\"Dihedral features: {success_counts['dihedral']} successful ({skipped_counts['dihedral']} skipped)\")\n",
    "        \n",
    "    if extract_mi:\n",
    "        print(f\"MI features: {success_counts['mi']} successful ({skipped_counts['mi']} skipped)\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_targets': len(target_ids),\n",
    "        'total_time': total_time,\n",
    "        'success_counts': success_counts,\n",
    "        'skipped_counts': skipped_counts,\n",
    "        'target_results': results\n",
    "    }\n",
    "    \n",
    "    with open(PROCESSED_DIR / 'validation_processing_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log initial memory usage for the entire run\n",
    "log_memory_usage(\"Initial memory before loading data\")\n",
    "\n",
    "# Load validationing data - both labels and sequences\n",
    "validation_labels_file = RAW_DIR / \"validation_labels.csv\"\n",
    "validation_sequences_file = RAW_DIR / \"validation_sequences.csv\"\n",
    "\n",
    "validation_labels = load_rna_data(validation_labels_file)\n",
    "validation_sequences = load_rna_data(validation_sequences_file)\n",
    "\n",
    "log_memory_usage(\"After loading validationing data\")\n",
    "\n",
    "if validation_labels is None:\n",
    "    print(\"Error loading validationing labels. Please make sure validation_labels.csv exists.\")\n",
    "elif validation_sequences is None:\n",
    "    print(\"Error loading validationing sequences. Please make sure validation_sequences.csv exists.\")\n",
    "else:\n",
    "    # Get unique target IDs from labels\n",
    "    target_ids = get_unique_target_ids(validation_labels)\n",
    "    \n",
    "    # Verify sequences exist for target IDs\n",
    "    seq_id_col = next((col for col in [\"target_id\", \"ID\", \"id\"] if col in validation_sequences.columns), None)\n",
    "    if seq_id_col:\n",
    "        # Check if IDs in sequence file contain underscores (indicating chain/structure info)\n",
    "        sequence_ids = validation_sequences[seq_id_col].astype(str).tolist()\n",
    "        \n",
    "        if any(\"_\" in str(id_val) for id_val in sequence_ids):\n",
    "            # Sequence IDs have same format (with underscore), do direct comparison\n",
    "            available_targets = set(sequence_ids)\n",
    "            target_with_sequences = [tid for tid in target_ids if tid in available_targets]\n",
    "        else:\n",
    "            # Sequence IDs are in a different format, try to match the base part\n",
    "            # For example, match \"R1107\" from sequences file with \"R1107_A\" from labels\n",
    "            available_targets = set()\n",
    "            for seq_id in sequence_ids:\n",
    "                # Add the ID as is and also try adding common chain identifiers\n",
    "                available_targets.add(seq_id)\n",
    "                available_targets.add(f\"{seq_id}_A\")  # Common chain identifier\n",
    "            \n",
    "            target_with_sequences = [tid for tid in target_ids if tid in available_targets]\n",
    "            \n",
    "            # If still no matches, try more flexible matching\n",
    "            if not target_with_sequences:\n",
    "                # Try to match beginning of target ID with sequence ID\n",
    "                target_with_sequences = []\n",
    "                for tid in target_ids:\n",
    "                    for seq_id in sequence_ids:\n",
    "                        if seq_id in tid or tid.startswith(seq_id):\n",
    "                            target_with_sequences.append(tid)\n",
    "                            break\n",
    "        \n",
    "        missing_sequences = len(target_ids) - len(target_with_sequences)\n",
    "        \n",
    "        if missing_sequences > 0:\n",
    "            print(f\"Warning: {missing_sequences} targets do not have sequences in validation_sequences.csv\")\n",
    "        \n",
    "        print(f\"Found sequences for {len(target_with_sequences)}/{len(target_ids)} targets\")\n",
    "        target_ids = target_with_sequences\n",
    "    # Limit for testing\n",
    "    if LIMIT is not None and LIMIT < len(target_ids):\n",
    "        print(f\"Limiting to first {LIMIT} targets for testing\")\n",
    "        target_ids = target_ids[:LIMIT]\n",
    "    \n",
    "    # Process targets\n",
    "    with MemoryTracker(\"Batch processing\"):\n",
    "        results = batch_process_targets(\n",
    "            target_ids,\n",
    "            extract_thermo=True,\n",
    "            extract_dihedral=True,\n",
    "            extract_mi=True\n",
    "        )\n",
    "    \n",
    "    # Verify features\n",
    "    print(\"\\nVerifying processed features for compatibility...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    verification_script = Path(\"../scripts/verify_feature_compatibility.py\")\n",
    "    if verification_script.exists():\n",
    "        try:\n",
    "            # Run the script as a subprocess\n",
    "            cmd = [sys.executable, str(verification_script), str(PROCESSED_DIR), \"--verbose\"]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            \n",
    "            # Print the output\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Check for errors\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Verification failed with exit code {result.returncode}\")\n",
    "                if result.stderr:\n",
    "                    print(f\"Error output: {result.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error running verification script: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Feature verification script not found at {verification_script}\")\n",
    "    \n",
    "    # Plot memory usage\n",
    "    log_memory_usage(\"Final memory usage\")\n",
    "    #plot_memory_usage(output_file=MEMORY_PLOTS_DIR / \"validation_memory_usage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validation_dihedral_extraction():\n",
    "    \"\"\"Test dihedral extraction on a single validation target.\"\"\"\n",
    "    if not target_ids or len(target_ids) == 0:\n",
    "        print(\"No target IDs available to test\")\n",
    "        return\n",
    "    \n",
    "    test_target = target_ids[0]\n",
    "    print(f\"Testing dihedral extraction for validation target: {test_target}\")\n",
    "    \n",
    "    # Load structure data\n",
    "    structure_data = load_structure_data(test_target)\n",
    "    if structure_data is None:\n",
    "        print(f\"❌ Failed to load structure data for {test_target}\")\n",
    "        return\n",
    "    \n",
    "    # Count coordinate sets\n",
    "    x_cols = [col for col in structure_data.columns if col.startswith('x_')]\n",
    "    print(f\"Found {len(x_cols)} coordinate sets: {', '.join(x_cols)}\")\n",
    "    \n",
    "    # Extract dihedral features\n",
    "    features = extract_dihedral_features_for_target(test_target, structure_data)\n",
    "    \n",
    "    if features is not None:\n",
    "        print(f\"✅ Successfully extracted dihedral features\")\n",
    "        print(f\"Number of structures: {features.get('num_structures', 'Unknown')}\")\n",
    "        print(\"Feature keys:\")\n",
    "        for key in list(features.keys())[:10]:  # Show first 10 keys\n",
    "            print(f\"  - {key}\")\n",
    "        if len(features) > 10:\n",
    "            print(f\"  ... and {len(features) - 10} more\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to extract dihedral features\")\n",
    "\n",
    "# Run the test on a single target before batch processing\n",
    "test_validation_dihedral_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader Compatibility Check\n",
    "\n",
    "Visualize and validate the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature verification script on our extracted features\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def verify_features():\n",
    "    \"\"\"Run the feature verification script on the processed data directory.\"\"\"\n",
    "    verification_script = Path(\"../scripts/verify_feature_compatibility.py\")\n",
    "    \n",
    "    # Check if the script exists\n",
    "    if not verification_script.exists():\n",
    "        print(f\"Error: Verification script not found at {verification_script}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Running feature verification script on {PROCESSED_DIR}\")\n",
    "    try:\n",
    "        # Run the script as a subprocess\n",
    "        cmd = [sys.executable, str(verification_script), str(PROCESSED_DIR), \"--verbose\"]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the output\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Check for errors\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Verification failed with exit code {result.returncode}\")\n",
    "            if result.stderr:\n",
    "                print(f\"Error output: {result.stderr}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error running verification script: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run the verification when features are available\n",
    "compatibility_check = verify_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_structure_data(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"Debug helper to diagnose structure loading issues.\"\"\"\n",
    "    print(f\"\\n=== DEBUGGING STRUCTURE DATA FOR {target_id} ===\")\n",
    "    \n",
    "    # Try to load the data\n",
    "    structure_data = load_structure_data(target_id, data_dir)\n",
    "    \n",
    "    if structure_data is None:\n",
    "        print(\"❌ No structure data found!\")\n",
    "        \n",
    "        # Check what label files exist\n",
    "        label_files = [\n",
    "            data_dir / \"validation_labels.csv\",\n",
    "            data_dir / \"test_labels.csv\",\n",
    "            data_dir / \"train_labels.csv\" \n",
    "        ]\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            if label_file.exists():\n",
    "                print(f\"📄 Found label file: {label_file}\")\n",
    "                \n",
    "                # Check structure of file\n",
    "                try:\n",
    "                    df = pd.read_csv(label_file)\n",
    "                    print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "                    print(f\"  - Sample IDs: {df['ID'].head(3).tolist() if 'ID' in df.columns else 'No ID column'}\")\n",
    "                    \n",
    "                    # Check if target ID exists in any similar form\n",
    "                    base_id = target_id.split('_')[0]\n",
    "                    matches = df[df['ID'].str.contains(base_id, regex=False)] if 'ID' in df.columns else None\n",
    "                    if matches is not None and len(matches) > 0:\n",
    "                        print(f\"  - Found {len(matches)} rows containing '{base_id}'\")\n",
    "                        print(f\"  - Sample IDs: {matches['ID'].head(3).tolist()}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  - Error reading file: {e}\")\n",
    "    else:\n",
    "        print(f\"✅ Found {len(structure_data)} residues for {target_id}\")\n",
    "        print(f\"Columns: {structure_data.columns.tolist()}\")\n",
    "        \n",
    "        # Check for coordinate columns\n",
    "        x_cols = [col for col in structure_data.columns if col.startswith('x_')]\n",
    "        y_cols = [col for col in structure_data.columns if col.startswith('y_')]\n",
    "        z_cols = [col for col in structure_data.columns if col.startswith('z_')]\n",
    "        \n",
    "        print(f\"Coordinate columns:\")\n",
    "        print(f\"  - X columns: {x_cols}\")\n",
    "        print(f\"  - Y columns: {y_cols}\")\n",
    "        print(f\"  - Z columns: {z_cols}\")\n",
    "        \n",
    "        # Check if proper coordinate columns exist\n",
    "        if not (x_cols and y_cols and z_cols):\n",
    "            print(\"❌ Missing coordinate columns!\")\n",
    "            \n",
    "        # Print the first few rows for inspection\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(structure_data.head(2))\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    return structure_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna3d-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
