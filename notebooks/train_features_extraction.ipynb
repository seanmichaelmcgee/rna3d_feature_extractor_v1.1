{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA 3D Train Features Extraction\n",
    "\n",
    "This notebook extracts all three types of features for RNA training data:\n",
    "1. Thermodynamic features from RNA sequences\n",
    "2. Pseudodihedral angle features from 3D coordinates\n",
    "3. Mutual Information features from Multiple Sequence Alignments (MSAs)\n",
    "\n",
    "This notebook works with training data that includes 3D structural information.\n",
    "\n",
    "## Dependencies\n",
    "- ViennaRNA (for thermodynamic features)\n",
    "- NumPy/SciPy/Pandas (core data processing)\n",
    "- Memory monitoring tools from src.analysis.memory_monitor\n",
    "- Feature extraction functions from src.analysis modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "\n",
    "# Ensure the parent directory is in the path so we can import our modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Import feature extraction modules\n",
    "from src.analysis.thermodynamic_analysis import extract_thermodynamic_features\n",
    "from src.analysis.dihedral_analysis import extract_dihedral_features\n",
    "from src.analysis.mutual_information import calculate_mutual_information, convert_mi_to_evolutionary_features\n",
    "from src.data.extract_features_simple import save_features_npz\n",
    "\n",
    "# Import memory monitoring utilities\n",
    "from src.analysis.memory_monitor import MemoryTracker, log_memory_usage, plot_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "# Define paths and parameters for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relative paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Output directories for each feature type\n",
    "THERMO_DIR = PROCESSED_DIR / \"thermo_features\"\n",
    "DIHEDRAL_DIR = PROCESSED_DIR / \"dihedral_features\"\n",
    "MI_DIR = PROCESSED_DIR / \"mi_features\"\n",
    "MEMORY_PLOTS_DIR = PROCESSED_DIR / \"memory_plots\"\n",
    "\n",
    "# Make sure all directories exist\n",
    "for directory in [RAW_DIR, PROCESSED_DIR, THERMO_DIR, DIHEDRAL_DIR, MI_DIR, MEMORY_PLOTS_DIR]:\n",
    "    directory.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "# Parameters\n",
    "LIMIT = 5  # Limit for testing; set to None to process all data\n",
    "VERBOSE = True  # Whether to print detailed progress\n",
    "\n",
    "# Auto-detect if running on Kaggle\n",
    "KAGGLE_MODE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "if KAGGLE_MODE:\n",
    "    print(\"Running in Kaggle environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Define utility functions for loading data and extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rna_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load RNA data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file containing RNA data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with RNA data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} entries from {csv_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_unique_target_ids(df, id_col=\"ID\"):\n",
    "    \"\"\"\n",
    "    Extract unique target IDs from dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with RNA data\n",
    "        id_col: Column containing IDs\n",
    "        \n",
    "    Returns:\n",
    "        List of unique target IDs\n",
    "    \"\"\"\n",
    "    # Extract target IDs (format: TARGET_ID_RESIDUE_NUM)\n",
    "    target_ids = []\n",
    "    for id_str in df[id_col]:\n",
    "        # Split the ID string and get the target ID part\n",
    "        parts = id_str.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            target_id = f\"{parts[0]}_{parts[1]}\"  # Take the first two parts (e.g., \"1SCL_A\")\n",
    "            target_ids.append(target_id)\n",
    "    \n",
    "    # Get unique target IDs\n",
    "    unique_targets = sorted(list(set(target_ids)))\n",
    "    print(f\"Found {len(unique_targets)} unique target IDs\")\n",
    "    return unique_targets\n",
    "\n",
    "def load_structure_data(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"\n",
    "    Load structure data for a given target from labels CSV.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        data_dir: Directory containing data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with structure coordinates or None if not found\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Define possible label files (train or validation)\n",
    "    label_files = [\n",
    "        data_dir / \"train_labels.csv\",\n",
    "        data_dir / \"validation_labels.csv\"\n",
    "    ]\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        if label_file.exists():\n",
    "            try:\n",
    "                print(f\"Looking for {target_id} in {label_file}\")\n",
    "                # Read the entire CSV file\n",
    "                all_data = pd.read_csv(label_file)\n",
    "                \n",
    "                # Filter rows for this target ID\n",
    "                target_data = all_data[all_data[\"ID\"].str.startswith(f\"{target_id}_\")]\n",
    "                \n",
    "                if len(target_data) > 0:\n",
    "                    print(f\"Found {len(target_data)} residues for {target_id}\")\n",
    "                    return target_data\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading from {label_file}: {e}\")\n",
    "    \n",
    "    print(f\"Could not find structure data for {target_id} in any labels file\")\n",
    "    return None\n",
    "\n",
    "def load_msa_data(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"\n",
    "    Load MSA data for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        data_dir: Directory containing MSA data\n",
    "        \n",
    "    Returns:\n",
    "        List of MSA sequences or None if not found\n",
    "    \"\"\"\n",
    "    # Try to find the MSA file\n",
    "    msa_paths = [\n",
    "        data_dir / \"MSA\" / f\"{target_id}.MSA.fasta\",\n",
    "        data_dir / f\"{target_id}.MSA.fasta\",\n",
    "        data_dir / \"alignments\" / f\"{target_id}.MSA.fasta\"\n",
    "    ]\n",
    "    \n",
    "    for path in msa_paths:\n",
    "        if path.exists():\n",
    "            print(f\"Loading MSA data from {path}\")\n",
    "            try:\n",
    "                # Parse FASTA file\n",
    "                sequences = []\n",
    "                current_seq = \"\"\n",
    "                 \n",
    "                with open(path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line.startswith('>'):\n",
    "                            if current_seq:\n",
    "                                sequences.append(current_seq)\n",
    "                                current_seq = \"\"\n",
    "                        else:\n",
    "                            current_seq += line\n",
    "                            \n",
    "                    # Add the last sequence\n",
    "                    if current_seq:\n",
    "                        sequences.append(current_seq)\n",
    "                \n",
    "                print(f\"Loaded {len(sequences)} sequences from MSA\")\n",
    "                return sequences\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading MSA data: {e}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"Could not find MSA data for {target_id}\")\n",
    "    return None\n",
    "\n",
    "def get_sequence_for_target(target_id, data_dir=RAW_DIR):\n",
    "    \"\"\"\n",
    "    Get RNA sequence for a target ID from the sequence file.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        data_dir: Directory containing sequence data\n",
    "        \n",
    "    Returns:\n",
    "        RNA sequence as string or None if not found\n",
    "    \"\"\"\n",
    "    # Try train_sequences.csv first (primary source for training sequences)\n",
    "    train_seq_path = data_dir / \"train_sequences.csv\"\n",
    "    if train_seq_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(train_seq_path)\n",
    "            # Try different possible column names for ID and sequence\n",
    "            id_cols = [\"target_id\", \"ID\", \"id\"]\n",
    "            seq_cols = [\"sequence\", \"Sequence\", \"seq\"]\n",
    "            \n",
    "            for id_col in id_cols:\n",
    "                if id_col in df.columns:\n",
    "                    for seq_col in seq_cols:\n",
    "                        if seq_col in df.columns:\n",
    "                            # Find the target in the dataframe\n",
    "                            target_row = df[df[id_col] == target_id]\n",
    "                            if len(target_row) > 0:\n",
    "                                sequence = target_row[seq_col].iloc[0]\n",
    "                                print(f\"Found sequence for {target_id} in train_sequences.csv, length: {len(sequence)}\")\n",
    "                                return sequence\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sequence data from {train_seq_path}: {e}\")\n",
    "    \n",
    "    # If not found in train_sequences.csv, try other sequence files\n",
    "    sequence_paths = [\n",
    "        data_dir / \"sequences.csv\",\n",
    "        data_dir / \"test_sequences.csv\",\n",
    "        data_dir / \"validation_sequences.csv\"\n",
    "    ]\n",
    "    \n",
    "    for path in sequence_paths:\n",
    "        if path.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                \n",
    "                # Try different possible column names\n",
    "                id_cols = [\"target_id\", \"ID\", \"id\"]\n",
    "                seq_cols = [\"sequence\", \"Sequence\", \"seq\"]\n",
    "                \n",
    "                for id_col in id_cols:\n",
    "                    if id_col in df.columns:\n",
    "                        for seq_col in seq_cols:\n",
    "                            if seq_col in df.columns:\n",
    "                                # Find the target in the dataframe\n",
    "                                target_row = df[df[id_col] == target_id]\n",
    "                                if len(target_row) > 0:\n",
    "                                    sequence = target_row[seq_col].iloc[0]\n",
    "                                    print(f\"Found sequence for {target_id} in {path.name}, length: {len(sequence)}\")\n",
    "                                    return sequence\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading sequence data from {path}: {e}\")\n",
    "    \n",
    "    # As a last resort, try to extract it from MSA data\n",
    "    print(f\"Could not find sequence in CSV files, trying MSA files as a last resort\")\n",
    "    msa_sequences = load_msa_data(target_id, data_dir)\n",
    "    if msa_sequences and len(msa_sequences) > 0:\n",
    "        # The first sequence in the MSA is typically the target sequence\n",
    "        sequence = msa_sequences[0]\n",
    "        print(f\"Found sequence for {target_id} in MSA file, length: {len(sequence)}\")\n",
    "        return sequence\n",
    "    \n",
    "    print(f\"Could not find sequence for {target_id} in any file\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "Define functions for extracting each type of feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_thermo_features_for_target(target_id, sequence=None):\n",
    "    \"\"\"\n",
    "    Extract thermodynamic features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        sequence: RNA sequence (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with thermodynamic features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting thermodynamic features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get sequence if not provided\n",
    "        if sequence is None:\n",
    "            sequence = get_sequence_for_target(target_id)\n",
    "            if sequence is None:\n",
    "                print(f\"Failed to get sequence for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Log initial memory usage\n",
    "        log_memory_usage(f\"Before thermo features for {target_id} (len={len(sequence)})\")\n",
    "        \n",
    "        # Calculate features with memory monitoring\n",
    "        print(f\"Calculating thermodynamic features for sequence of length {len(sequence)}\")\n",
    "        with MemoryTracker(f\"Thermodynamic features calculation for {target_id}\"):\n",
    "            features = extract_thermodynamic_features(sequence)\n",
    "        \n",
    "        # Save features\n",
    "        output_file = THERMO_DIR / f\"{target_id}_thermo_features.npz\"\n",
    "        features['target_id'] = target_id\n",
    "        features['sequence'] = sequence\n",
    "        \n",
    "        with MemoryTracker(\"Saving thermodynamic features\"):\n",
    "            save_features_npz(features, output_file)\n",
    "        \n",
    "        # Log final memory usage\n",
    "        log_memory_usage(f\"After thermo features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted thermodynamic features in {elapsed_time:.2f} seconds\")\n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting thermodynamic features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def extract_dihedral_features_for_target(target_id, structure_data=None):\n",
    "    \"\"\"\n",
    "    Extract pseudodihedral angle features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure coordinates (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with dihedral features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting dihedral features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get structure data if not provided\n",
    "        if structure_data is None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "            if structure_data is None:\n",
    "                print(f\"Failed to get structure data for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Check if we have at least 4 residues (required for dihedral angles)\n",
    "        if len(structure_data) < 4:\n",
    "            print(f\"Not enough residues ({len(structure_data)}) for {target_id}, minimum 4 required for dihedral angles\")\n",
    "            return None\n",
    "        \n",
    "        # Check if we have the necessary coordinate columns\n",
    "        required_cols = ['x_1', 'y_1', 'z_1']\n",
    "        if not all(col in structure_data.columns for col in required_cols):\n",
    "            # Try to find alternative column names\n",
    "            alt_x_cols = [col for col in structure_data.columns if col.startswith('x_')]\n",
    "            if alt_x_cols:\n",
    "                x_col = alt_x_cols[0]\n",
    "                y_col = x_col.replace('x_', 'y_')\n",
    "                z_col = x_col.replace('x_', 'z_')\n",
    "                \n",
    "                # Rename columns for compatibility\n",
    "                if all(col in structure_data.columns for col in [x_col, y_col, z_col]):\n",
    "                    structure_data = structure_data.rename(columns={\n",
    "                        x_col: 'x_1',\n",
    "                        y_col: 'y_1',\n",
    "                        z_col: 'z_1'\n",
    "                    })\n",
    "                    print(f\"Renamed columns {x_col}, {y_col}, {z_col} to x_1, y_1, z_1\")\n",
    "                else:\n",
    "                    print(f\"Missing coordinate columns for {target_id}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Missing coordinate columns for {target_id}\")\n",
    "                return None\n",
    "        \n",
    "        # Log memory before dihedral calculation\n",
    "        log_memory_usage(f\"Before dihedral features for {target_id} (residues={len(structure_data)})\")\n",
    "        \n",
    "        # Calculate dihedral features\n",
    "        output_file = DIHEDRAL_DIR / f\"{target_id}_dihedral_features.npz\"\n",
    "        print(f\"Calculating dihedral features for {len(structure_data)} residues\")\n",
    "        \n",
    "        with MemoryTracker(f\"Dihedral features calculation for {target_id}\"):\n",
    "            dihedral_features = extract_dihedral_features(structure_data, output_file=output_file, include_raw_angles=True)\n",
    "        \n",
    "        # Log memory after dihedral calculation\n",
    "        log_memory_usage(f\"After dihedral features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted dihedral features in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Add target ID\n",
    "        dihedral_features['target_id'] = target_id\n",
    "        return dihedral_features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dihedral features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def extract_mi_features_for_target(target_id, structure_data=None, msa_sequences=None):\n",
    "    \"\"\"\n",
    "    Extract Mutual Information features for a given target.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        structure_data: DataFrame with structure data for correlation calculation (optional)\n",
    "        msa_sequences: List of MSA sequences (optional, will be loaded if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MI features or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"Extracting MI features for {target_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get MSA sequences if not provided\n",
    "        if msa_sequences is None:\n",
    "            msa_sequences = load_msa_data(target_id)\n",
    "            if msa_sequences is None or len(msa_sequences) < 2:\n",
    "                print(f\"Failed to get MSA data for {target_id} or not enough sequences\")\n",
    "                return None\n",
    "        \n",
    "        # Get structure data if not provided (for correlation calculation)\n",
    "        if structure_data is None and target_id is not None:\n",
    "            structure_data = load_structure_data(target_id)\n",
    "        \n",
    "        # Log memory before MI calculation\n",
    "        sequence_length = len(msa_sequences[0]) if msa_sequences else 0\n",
    "        msa_size = len(msa_sequences) if msa_sequences else 0\n",
    "        log_memory_usage(f\"Before MI features for {target_id} (seq_len={sequence_length}, msa_size={msa_size})\")\n",
    "        \n",
    "        # Calculate MI (this may take some time for large MSAs)\n",
    "        print(f\"Calculating MI for {len(msa_sequences)} sequences\")\n",
    "        with MemoryTracker(f\"MI calculation for {target_id}\"):\n",
    "            mi_result = calculate_mutual_information(msa_sequences, verbose=VERBOSE)\n",
    "        \n",
    "        if mi_result is None:\n",
    "            print(f\"Failed to calculate MI for {target_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to evolutionary features\n",
    "        output_file = MI_DIR / f\"{target_id}_mi_features.npz\"\n",
    "        \n",
    "        # If we have structure data, use it for correlation calculation\n",
    "        if structure_data is not None:\n",
    "            print(f\"Converting MI to evolutionary features with structural correlation\")\n",
    "            with MemoryTracker(f\"MI-structure correlation for {target_id}\"):\n",
    "                features = convert_mi_to_evolutionary_features(mi_result, structure_data, output_file=output_file)\n",
    "        else:\n",
    "            print(f\"Converting MI to evolutionary features without structural correlation\")\n",
    "            features = mi_result\n",
    "            \n",
    "            # Save manually if convert_mi_to_evolutionary_features wasn't used\n",
    "            if output_file is not None:\n",
    "                with MemoryTracker(f\"Saving MI features for {target_id}\"):\n",
    "                    np.savez_compressed(output_file, **features)\n",
    "                print(f\"Saved MI features to {output_file}\")\n",
    "        \n",
    "        # Log memory after MI calculation\n",
    "        log_memory_usage(f\"After MI features for {target_id}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Extracted MI features in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Add target ID\n",
    "        features['target_id'] = target_id\n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting MI features for {target_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process multiple targets in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_target(target_id, extract_thermo=True, extract_dihedral=True, extract_mi=True):\n",
    "    \"\"\"\n",
    "    Process a single target, extracting all requested feature types.\n",
    "    \n",
    "    Args:\n",
    "        target_id: Target ID\n",
    "        extract_thermo: Whether to extract thermodynamic features\n",
    "        extract_dihedral: Whether to extract dihedral features\n",
    "        extract_mi: Whether to extract MI features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each feature type\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing target: {target_id}\")\n",
    "    results = {'target_id': target_id}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load common data that might be used by multiple feature types\n",
    "    sequence = get_sequence_for_target(target_id) if extract_thermo else None\n",
    "    structure_data = load_structure_data(target_id) if extract_dihedral or extract_mi else None\n",
    "    msa_sequences = load_msa_data(target_id) if extract_mi else None\n",
    "    \n",
    "    # Extract thermodynamic features\n",
    "    if extract_thermo:\n",
    "        thermo_file = THERMO_DIR / f\"{target_id}_thermo_features.npz\"\n",
    "        \n",
    "        if thermo_file.exists():\n",
    "            print(f\"Thermodynamic features already exist for {target_id}\")\n",
    "            results['thermo'] = {'success': True, 'skipped': True}\n",
    "        else:\n",
    "            thermo_features = extract_thermo_features_for_target(target_id, sequence)\n",
    "            results['thermo'] = {'success': thermo_features is not None}\n",
    "    \n",
    "    # Extract dihedral features\n",
    "    if extract_dihedral:\n",
    "        dihedral_file = DIHEDRAL_DIR / f\"{target_id}_dihedral_features.npz\"\n",
    "        \n",
    "        if dihedral_file.exists():\n",
    "            print(f\"Dihedral features already exist for {target_id}\")\n",
    "            results['dihedral'] = {'success': True, 'skipped': True}\n",
    "        else:\n",
    "            dihedral_features = extract_dihedral_features_for_target(target_id, structure_data)\n",
    "            results['dihedral'] = {'success': dihedral_features is not None}\n",
    "    \n",
    "    # Extract MI features\n",
    "    if extract_mi:\n",
    "        mi_file = MI_DIR / f\"{target_id}_mi_features.npz\"\n",
    "        \n",
    "        if mi_file.exists():\n",
    "            print(f\"MI features already exist for {target_id}\")\n",
    "            results['mi'] = {'success': True, 'skipped': True}\n",
    "        else:\n",
    "            mi_features = extract_mi_features_for_target(target_id, structure_data, msa_sequences)\n",
    "            results['mi'] = {'success': mi_features is not None}\n",
    "    \n",
    "    # Calculate total time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    results['elapsed_time'] = elapsed_time\n",
    "    print(f\"Completed processing {target_id} in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def batch_process_targets(target_ids, extract_thermo=True, extract_dihedral=True, extract_mi=True):\n",
    "    \"\"\"\n",
    "    Process multiple targets in batch mode.\n",
    "    \n",
    "    Args:\n",
    "        target_ids: List of target IDs\n",
    "        extract_thermo: Whether to extract thermodynamic features\n",
    "        extract_dihedral: Whether to extract dihedral features\n",
    "        extract_mi: Whether to extract MI features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each target\n",
    "    \"\"\"\n",
    "    print(f\"Starting batch processing for {len(target_ids)} targets\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = {}\n",
    "    for i, target_id in enumerate(target_ids):\n",
    "        print(f\"\\nProcessing target {i+1}/{len(target_ids)}: {target_id}\")\n",
    "        \n",
    "        # Process the target\n",
    "        target_results = process_target(\n",
    "            target_id, \n",
    "            extract_thermo=extract_thermo, \n",
    "            extract_dihedral=extract_dihedral, \n",
    "            extract_mi=extract_mi\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[target_id] = target_results\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    success_counts = {\n",
    "        'thermo': sum(1 for r in results.values() if 'thermo' in r and r['thermo']['success']),\n",
    "        'dihedral': sum(1 for r in results.values() if 'dihedral' in r and r['dihedral']['success']),\n",
    "        'mi': sum(1 for r in results.values() if 'mi' in r and r['mi']['success'])\n",
    "    }\n",
    "    \n",
    "    skipped_counts = {\n",
    "        'thermo': sum(1 for r in results.values() if 'thermo' in r and r['thermo'].get('skipped', False)),\n",
    "        'dihedral': sum(1 for r in results.values() if 'dihedral' in r and r['dihedral'].get('skipped', False)),\n",
    "        'mi': sum(1 for r in results.values() if 'mi' in r and r['mi'].get('skipped', False))\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nBatch processing complete!\")\n",
    "    print(f\"Total targets: {len(target_ids)}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if extract_thermo:\n",
    "        print(f\"Thermodynamic features: {success_counts['thermo']} successful ({skipped_counts['thermo']} skipped)\")\n",
    "        \n",
    "    if extract_dihedral:\n",
    "        print(f\"Dihedral features: {success_counts['dihedral']} successful ({skipped_counts['dihedral']} skipped)\")\n",
    "        \n",
    "    if extract_mi:\n",
    "        print(f\"MI features: {success_counts['mi']} successful ({skipped_counts['mi']} skipped)\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_targets': len(target_ids),\n",
    "        'total_time': total_time,\n",
    "        'success_counts': success_counts,\n",
    "        'skipped_counts': skipped_counts,\n",
    "        'target_results': results\n",
    "    }\n",
    "    \n",
    "    with open(PROCESSED_DIR / 'train_processing_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log initial memory usage for the entire run\n",
    "log_memory_usage(\"Initial memory before loading data\")\n",
    "\n",
    "# Load training data - both labels and sequences\n",
    "train_labels_file = RAW_DIR / \"train_labels.csv\"\n",
    "train_sequences_file = RAW_DIR / \"train_sequences.csv\"\n",
    "\n",
    "train_labels = load_rna_data(train_labels_file)\n",
    "train_sequences = load_rna_data(train_sequences_file)\n",
    "\n",
    "log_memory_usage(\"After loading training data\")\n",
    "\n",
    "if train_labels is None:\n",
    "    print(\"Error loading training labels. Please make sure train_labels.csv exists.\")\n",
    "elif train_sequences is None:\n",
    "    print(\"Error loading training sequences. Please make sure train_sequences.csv exists.\")\n",
    "else:\n",
    "    # Get unique target IDs from labels\n",
    "    target_ids = get_unique_target_ids(train_labels)\n",
    "    \n",
    "    # Verify sequences exist for target IDs\n",
    "    seq_id_col = next((col for col in [\"target_id\", \"ID\", \"id\"] if col in train_sequences.columns), None)\n",
    "    if seq_id_col:\n",
    "        available_targets = set(train_sequences[seq_id_col])\n",
    "        target_with_sequences = [tid for tid in target_ids if tid in available_targets]\n",
    "        missing_sequences = len(target_ids) - len(target_with_sequences)\n",
    "        \n",
    "        if missing_sequences > 0:\n",
    "            print(f\"Warning: {missing_sequences} targets do not have sequences in train_sequences.csv\")\n",
    "        \n",
    "        print(f\"Found sequences for {len(target_with_sequences)}/{len(target_ids)} targets\")\n",
    "        target_ids = target_with_sequences\n",
    "    \n",
    "    # Limit for testing\n",
    "    if LIMIT is not None and LIMIT < len(target_ids):\n",
    "        print(f\"Limiting to first {LIMIT} targets for testing\")\n",
    "        target_ids = target_ids[:LIMIT]\n",
    "    \n",
    "    # Process targets\n",
    "    with MemoryTracker(\"Batch processing\"):\n",
    "        results = batch_process_targets(\n",
    "            target_ids,\n",
    "            extract_thermo=True,\n",
    "            extract_dihedral=True,\n",
    "            extract_mi=True\n",
    "        )\n",
    "    \n",
    "    # Verify features\n",
    "    print(\"\\nVerifying processed features for compatibility...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    verification_script = Path(\"../scripts/verify_feature_compatibility.py\")\n",
    "    if verification_script.exists():\n",
    "        try:\n",
    "            # Run the script as a subprocess\n",
    "            cmd = [sys.executable, str(verification_script), str(PROCESSED_DIR), \"--verbose\"]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            \n",
    "            # Print the output\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Check for errors\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Verification failed with exit code {result.returncode}\")\n",
    "                if result.stderr:\n",
    "                    print(f\"Error output: {result.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error running verification script: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Feature verification script not found at {verification_script}\")\n",
    "    \n",
    "    # Plot memory usage\n",
    "    log_memory_usage(\"Final memory usage\")\n",
    "    #plot_memory_usage(output_file=MEMORY_PLOTS_DIR / \"train_memory_usage.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Profiling for Different RNA Lengths\n",
    "\n",
    "This section includes memory profiling for different RNA sequence lengths to understand resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and profile sequences of different lengths\n",
    "def profile_memory_for_lengths(lengths=[100, 500, 1000, 2000, 3000]):\n",
    "    \"\"\"Profile memory usage for different RNA sequence lengths.\"\"\"\n",
    "    from src.analysis.memory_monitor import profile_rna_length_memory\n",
    "    \n",
    "    # Reset memory history\n",
    "    from src.analysis.memory_monitor import memory_history\n",
    "    memory_history['timestamps'] = []\n",
    "    memory_history['usage_gb'] = []\n",
    "    memory_history['labels'] = []\n",
    "    \n",
    "    # Create output directory for memory plots\n",
    "    memory_dir = PROCESSED_DIR / \"memory_profiling\"\n",
    "    memory_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Run profiling\n",
    "    results = profile_rna_length_memory(\n",
    "        seq_lengths=lengths,\n",
    "        output_dir=memory_dir\n",
    "    )\n",
    "    \n",
    "    # Create a summary table\n",
    "    print(\"\\nMemory Usage by Sequence Length:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Length (nt)':<15} {'Peak Memory (GB)':<20} {'Time (s)':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for length in sorted(results.keys()):\n",
    "        print(f\"{length:<15} {results[length]['peak_memory_gb']:<20.2f} {results[length]['processing_time']:<15.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment and run this cell to profile memory usage for different sequence lengths\n",
    "# This might take some time to complete\n",
    "# profile_results = profile_memory_for_lengths(lengths=[100, 500, 1000, 2000, 3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader Compatibility Check\n",
    "\n",
    "Let's verify that our features are compatible with the downstream data loader requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature verification script on our extracted features\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def verify_features():\n",
    "    \"\"\"Run the feature verification script on the processed data directory.\"\"\"\n",
    "    verification_script = Path(\"../scripts/verify_feature_compatibility.py\")\n",
    "    \n",
    "    # Check if the script exists\n",
    "    if not verification_script.exists():\n",
    "        print(f\"Error: Verification script not found at {verification_script}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Running feature verification script on {PROCESSED_DIR}\")\n",
    "    try:\n",
    "        # Run the script as a subprocess\n",
    "        cmd = [sys.executable, str(verification_script), str(PROCESSED_DIR), \"--verbose\"]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the output\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Check for errors\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Verification failed with exit code {result.returncode}\")\n",
    "            if result.stderr:\n",
    "                print(f\"Error output: {result.stderr}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error running verification script: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run the verification when features are available\n",
    "compatibility_check = verify_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and profile sequences of different lengths\n",
    "def profile_memory_for_lengths(lengths=[100, 500, 1000, 2000, 3000]):\n",
    "    \"\"\"Profile memory usage for different RNA sequence lengths.\"\"\"\n",
    "    from src.analysis.memory_monitor import profile_rna_length_memory\n",
    "    \n",
    "    # Reset memory history\n",
    "    from src.analysis.memory_monitor import memory_history\n",
    "    memory_history['timestamps'] = []\n",
    "    memory_history['usage_gb'] = []\n",
    "    memory_history['labels'] = []\n",
    "    \n",
    "    # Create output directory for memory plots\n",
    "    memory_dir = PROCESSED_DIR / \"memory_profiling\"\n",
    "    memory_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Run profiling\n",
    "    results = profile_rna_length_memory(\n",
    "        seq_lengths=lengths,\n",
    "        output_dir=memory_dir\n",
    "    )\n",
    "    \n",
    "    # Create a summary table\n",
    "    print(\"\\nMemory Usage by Sequence Length:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Length (nt)':<15} {'Peak Memory (GB)':<20} {'Time (s)':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for length in sorted(results.keys()):\n",
    "        print(f\"{length:<15} {results[length]['peak_memory_gb']:<20.2f} {results[length]['processing_time']:<15.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment and run this cell to profile memory usage for different sequence lengths\n",
    "# This might take some time to complete\n",
    "# profile_results = profile_memory_for_lengths(lengths=[100, 500, 1000, 2000, 3000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna3d-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
